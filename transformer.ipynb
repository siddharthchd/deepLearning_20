{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsgDISqZwfkalGbr1PXgiQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddharthchd/deepLearning_20/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxzHpxxykC7-"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ODtsNl7kMqh"
      },
      "source": [
        "# Self-Attention\n",
        "\n",
        "We have a set of t inputs $\\{x_{i}\\}^{t}_{i=1} \\in \\mathbb{R}^{n}$\n",
        "\n",
        "The x's can be considered a matrix with n rows and t cols\n",
        "\n",
        "$X = \\begin{bmatrix} x_{1}&x_{2}&\\cdots&x_{t} \\end{bmatrix} \\in \\mathbb{R}^{n\\times t}$\n",
        "\n",
        "Hidden representation is a linear combination of the column vectors $x_{i}$ : $h = \\alpha_{1}x_{1} + \\alpha_{2}x_{2} + \\cdots + \\alpha_{t}x_{t} = Xa \\in \\mathbb{R}^{n}$\n",
        "\n",
        "\"Hard\" attention: $\\lvert\\lvert{a}\\rvert\\rvert_{0} = 1$\n",
        "\n",
        "i.e., $a$ is a one-hot vector $\\rightarrow$ multiplication by $X$ is a selection of columns\n",
        "select one element of the set\n",
        "\n",
        "\"Soft\" attention: $\\lvert\\lvert{a}\\rvert\\rvert_{1} = 1$\n",
        "\n",
        "i.e., constraint is that the summation of elements of $a$, the $\\alpha$'s, sum to $1$\n",
        "\n",
        "Where do the $a$'s come from?\n",
        "$a = [soft]\\arg\\max_{\\beta}(X^{T}x)\\in\\mathbb{R}^{t}$\n",
        "\n",
        "$a$ is the value of the scalar product of input vector $x$ with every other vector in the set (denoted $X$). Every element in the final product is the scalar product of all elements against a given $x$.\n",
        "\n",
        "n.b.: $\\beta$ is the parameter of the soft argmax (usu. referred to as \"softmax\") (in energy terms, the inverse of the temperature, $\\exp$ of argument divided by sum of $\\exp$s). It's there whenever you have soft argmax; $\\beta$ is usually set to one so you don't see it but it's inside the $\\exp$\n",
        "\n",
        "use argmax -> one-hot encoding\n",
        "\n",
        "soft argmax-> exponential divided by summation of all exponentials\n",
        "\n",
        "A set of $x$'s implies a set of $a's$; Stack the vectors $a$ in matrix $A \\in \\mathbb{R}^{t\\times t}$\n",
        "\n",
        "$a$ has size $t$ for $t$ rows in $x^{T}$\n",
        "\n",
        "a set of $a$'s implies a set of $h$'s: $H \\in \\mathbb{R}^{n\\times t}$\n",
        "\n",
        "Finally: $H = XA \\in\\mathbb{R}$\n",
        "$H$ is a linear combination of the elements of $X$ using the factors in the columns of $A$\n",
        "\n",
        "Overall: mix the components of the set of $x$'s by using these coefficients which are computed using the soft argmax, where each component has a score of cosine similarity (dot product) of a given $x$ against the set of $x$'s\n",
        "\n",
        "# Key-value store\n",
        "\n",
        "Conceptually, we are checking how aligned is the query against all the values in the dataset (compute how matching the dataset values are with respect to your query)\n",
        "\n",
        "We can retrieve the single maximum matching element with argmax OR use soft argmax to find a probability distribution - can retrieve things in order/have a sequence of decreasingly less correlated/similar items\n",
        "\n",
        "Queries, keys, and values are rotations of input $x$: $q = W_q x; k = W_k x; v = W_v x$; these rotations $W_q, W_k, W_v$ are training parameters\n",
        "\n",
        "Attention is completely based on orientation; the only nonlinearity is the soft argmax for probability distribution\n",
        "\n",
        "$q, k$ must have the same dimension; $v$ is the returned value/content associated with a given key\n",
        "\n",
        "Given that we have a set of x's we'll have a set of queries, keys, values, we can make a matrix stacking them all up; matrix has $t$ cols of row vectors of size $d$\n",
        "\n",
        "Next: check  query against all keys - transpose $K$ against every $q$ query\n",
        "\n",
        "This returns $t$ scores which constitute a probability distribution over the space of possible matching sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MJnaIoNkp1Q"
      },
      "source": [
        "## Transformer Model\n",
        "\n",
        "### Multi-head attention module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ePyO-cIlNst"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "nn_Softargmax = nn.Softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sX04z9ElcN6"
      },
      "source": [
        "# Multiple heads : allows for multiple properties per query\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, p, d_input = None):\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        if d_input is None:\n",
        "            d_xq = d_xk = d_xv = d_model\n",
        "        else:\n",
        "            d_xq, d_xk, d_xv = d_input\n",
        "\n",
        "        # Make sure that the embedding dimension of model is a multiple of number of heads\n",
        "        assert d_model % self.num_heads == 0\n",
        "        \n",
        "        self.d_k = d_model // self.num_heads\n",
        "\n",
        "        # Matrices allowing to rotate current input\n",
        "        # (These are still of dimension d_model. They will be split into number of heads)\n",
        "        self.W_q = nn.Linear(d_xq, d_model, bias = False)\n",
        "        self.W_k = nn.Linear(d_xk, d_model, bias = False)\n",
        "        self.W_v = nn.Linear(d_xv, d_model, bias = False)\n",
        "\n",
        "        # Outputs of all sub-layers need to be of dimension d_model\n",
        "        self.W_h = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "\n",
        "        batch_size = Q.size(0)\n",
        "        k_length = K.size(-2)\n",
        "\n",
        "        # Scaling by d_k so that the soft(arg)max doesn't saturate\n",
        "        Q = Q / np.sqrt(self.d_k)\n",
        "\n",
        "        # Multiplication between one query and all keys\n",
        "        scores = torch.matmul(Q, K.transpose(2, 3)) # (bs, n_heads, q_length, k_length)\n",
        "\n",
        "        # Compute the mixing coefficients\n",
        "        A = nn_Softargmax(dim = -1)(scores) # (bs, n_heads, q_length, k_length)\n",
        "\n",
        "        # Get the weighted average of the values - multiply mixing coeff with V matrix\n",
        "        H = torch.matmul(A, V)  # (bs, n_heads, q_length, dim_per_head)\n",
        "\n",
        "        return H, A\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension into (heads X depth)\n",
        "        Return after transpose to put in shape (batch_size X num_heads X seq_length X d_k)\n",
        "        \"\"\"\n",
        "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def group_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Combine the heads again to get (batch_size X seq_length X (num_heads times d_k))\n",
        "        \"\"\"\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "\n",
        "    def forward(self, X_q, X_k, X_v):\n",
        "\n",
        "        batch_size, seq_length, dim = X_q.size()\n",
        "\n",
        "        # Apply W transformation (learned rotation of x input), then split into num_heads\n",
        "        Q = self.split_heads(self.W_q(X_q), batch_size) # (bs, n_heads, q_length, dim_per_head)\n",
        "        K = self.split_heads(self.W_k(X_k), batch_size) # (bs, n_heads, k_length, dim_per_head)\n",
        "        V = self.split_heads(self.W_v(X_v), batch_size) # (bs, n_heads, v_length, dim_per_head)\n",
        "\n",
        "        # Compute the scaled dot product between one query against all keys\n",
        "        # i.e. Calculate the attention weights fro each of the heads\n",
        "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "        # Put all the heads back together by concat\n",
        "        H_cat = self.group_heads(H_cat, batch_size) # (bs, q_length, dim)\n",
        "\n",
        "        # Final linear layer\n",
        "        H = self.W_h(H_cat) # (bs, q_length, dim)\n",
        "\n",
        "        return H, A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CBm4CX1m4UV"
      },
      "source": [
        "### Check how the self-attention mechanism works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFHsVKKoqlOU"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model = 512, num_heads = 8, p = 0)\n",
        "\n",
        "def print_out(Q, K, V):\n",
        "\n",
        "    temp_out, temp_attn = temp_mha.scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "    print('Attention weights are : ', temp_attn.squeeze())\n",
        "    print('Output is : ', temp_out.squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSOiJvjrq79O",
        "outputId": "f59307a4-c249-4b15-e721-1b0cdbaf3144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_K = torch.tensor(\n",
        "    [[10, 0, 0],\n",
        "     [ 0,10, 0],\n",
        "     [ 0, 0,10],\n",
        "     [ 0, 0,10]]\n",
        ").float()[None, None]\n",
        "\n",
        "test_V = torch.tensor(\n",
        "    [[   1,0,0],\n",
        "     [  10,0,0],\n",
        "     [ 100,5,0],\n",
        "     [1000,6,0]]\n",
        ").float()[None, None]\n",
        "\n",
        "test_Q = torch.tensor(\n",
        "    [[0, 10, 0]]\n",
        ").float()[None, None]\n",
        "\n",
        "print_out(test_Q, test_K, test_V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are :  tensor([3.7266e-06, 9.9999e-01, 3.7266e-06, 3.7266e-06])\n",
            "Output is :  tensor([1.0004e+01, 4.0993e-05, 0.0000e+00])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFAbwbm4q_Wl"
      },
      "source": [
        "## 1D convolution with `kernel_size = 1`\n",
        "\n",
        "This is equivalent to an MLP with one hidden layer and ReLU activation applied to each and every element in the set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deJd9Xh_rnZ-"
      },
      "source": [
        "# Element wise feedforward = 1d Convolution with kernel size 1\n",
        "# linear layer maps a representation to some other representation ( is a transformation)\n",
        "# convolution maps one set to another set - which is what we arr actually doing here \n",
        "# applt same linear transform to every elemnt in a sequence\n",
        "\n",
        "# conv hidden layer is applied to every component in the set - every element treated separately\n",
        "# if you apply same linear layer to every element in a sequence -> that's a convolution\n",
        "# in practice, implementations generally use a linear layer\n",
        "\n",
        "class CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, hidden_dim, p):\n",
        "\n",
        "        super().__init__()\n",
        "        self.k1convL1 = nn.Linear(d_model, hidden_dim)\n",
        "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.k1convL1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.k1convL2(x)\n",
        "\n",
        "        return x\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naqt1S6xu03A"
      },
      "source": [
        "## Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mtVlAdku3t-"
      },
      "source": [
        "# Components of encoder block : \n",
        "# 1 : self attention\n",
        "# 2 : convolution - MLP applied to every element in the set\n",
        "\n",
        "class EncodeLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, conv_hidden_dim, p = 0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads, p)\n",
        "        self.cnn = CNN(d_model, conv_hidden_dim, p)\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(normalized_shape = d_model, eps = 1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(normalized_shape = d_model, eps = 1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Multi-head attention\n",
        "        attention_output, _ = self.mha(x, x, x) # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # Layer norm after adding the residual connection\n",
        "        out1 = self.layernorm1(x + attention_output)    # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # Feed forward\n",
        "        cnn_output = self.cnn(out1) # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        # Second layer norm after adding residual connection\n",
        "        out2 = self.layernorm2(out1 + cnn_output)   # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo1xoNqnyisd"
      },
      "source": [
        "## Positional Embeddings\n",
        "\n",
        "see https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
        "\n",
        "Until here, basically like a bag of words, but also want to send information about what position the item takes. So far, the encoder, transformer, attention are all permutation equivariant. For sentences, we need to account for the order of words. We can add information about the position to the initial encoding of the words. In this way, the encoding is not integrated into the model itself, but rather enhances the input to the model with information about its own position. Since the Transformer architecture is equipped with residual connections, the information from the input of the model (containing positional embeddings) can propagate to further layers (further layers remain aware of position).\n",
        "\n",
        "Some criteria for position-sensitive encoding:\n",
        "- Should output a unique encoding for each time-step/word position in a sentence\n",
        "- Distance between any two time-steps should be consistent across sentences with different lengths\n",
        "- Model should generalize to longer sentences without any efforts; values should be bounded\n",
        "- Must be deterministic\n",
        "\n",
        "Let $t$ be the desired position in an input sentence, $p_{t} \\in \\mathbb{R}^{d}$ be its corresponding encoding, and $d$ be the encoding dimension, same as the word embedding dimension - the positional embedding is a transformation of the word embedding:\n",
        "\n",
        "$$\\psi^{\\prime}(w_t) = \\psi(w_t)+p_t$$\n",
        "\n",
        "Sinusoidal positional embeddings:\n",
        "\n",
        "\\begin{aligned}\n",
        "E(p, 2i)    &= \\sin(p / 10000^{2i / d}) \\\\\n",
        "E(p, 2i+1) &= \\cos(p / 10000^{2i / d})\n",
        "\\end{aligned}\n",
        "\n",
        "- the positional embedding $p_t$ as a vector containing pairs of sines and cosines\n",
        "\n",
        "- represents $p_{t+\\phi}$ as a linear function of $p_t$ for any fixed offset $\\phi$ - the sines and cosines implement a rotation transformation\n",
        "\n",
        "- position as the frequency of flip in value when incrementing, which varies depending on the bit position -> sinusoidal functions as the continuous version of alternating bits\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHO8auJmylv2"
      },
      "source": [
        "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
        "\n",
        "    theta = np.array([\n",
        "            [p / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
        "            for p in range(nb_p)\n",
        "    ])\n",
        "    E[:, 0 :: 2] = torch.FloatTensor(np.sin(theta[:, 0 :: 2]))\n",
        "    E[:, 1 :: 2] = torch.FloatTensor(np.cos(theta[:, 1 :: 2]))\n",
        "    E.detach_()\n",
        "    E.requires_grad = False\n",
        "    E = E.to(device)\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, vocab_size, max_position_embeddings, p):\n",
        "\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx = 1) # a simple lookup table that stores embeddings of a fixed dictionary and size\n",
        "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
        "        create_sinusoidal_embeddings(\n",
        "            nb_p = max_position_embeddings,\n",
        "            dim = d_model,\n",
        "            E = self.position_embeddings.weight\n",
        "        )\n",
        "\n",
        "        self.LayerNorm = nn.LayerNorm(d_model, eps = 1e-12)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype = torch.long, device = input_ids.device)  # (max_seq_length)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "        # Get word embeddings for each input\n",
        "        word_embeddings = self.word_embeddings(input_ids)   # (bs, max_seq_length, dim)\n",
        "\n",
        "        # Get position embeddings for each position id\n",
        "        position_embeddings = self.position_embeddings(position_ids)    # (bs, max_seq_length, dim)\n",
        "\n",
        "        # Add them both\n",
        "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
        "\n",
        "        # Layer norm\n",
        "        embeddings = self.LayerNorm(embeddings) # (bs, max_seq_length, dim)\n",
        "\n",
        "        return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP8SyAIz6yHL"
      },
      "source": [
        "## Overall Encoder\n",
        "\n",
        "### (Blocks of N Encoder Layers + Positional encoding + Input embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUBEiMbG7Q_R"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, ff_hidden_dim, input_vocab_size, maximum_position_encoding, p = 0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Apply permutation-sensitive embeddings\n",
        "        self.embedding = Embeddings(d_model, input_vocab_size, maximum_position_encoding, p)\n",
        "\n",
        "        self.enc_layers = nn.ModuleList()\n",
        "\n",
        "        for _ in range(num_layers):\n",
        "\n",
        "            self.enc_layers.append(EncodeLayer(d_model, num_heads, ff_hidden_dim, p))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
        "\n",
        "        # Stack multiple to make network 'more powerful'\n",
        "        # append several encoders together\n",
        "        for i in range(self.num_layers):\n",
        "\n",
        "            x = self.enc_layers[i](x)\n",
        "\n",
        "        return x    # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEgSvrb6GFGx"
      },
      "source": [
        "import torchtext.data as data\n",
        "import torchtext.datasets as datasets"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzJXULBcGYZH",
        "outputId": "ed5ea1d4-d1c3-4cdc-bc05-bbb4b08be8e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "max_len = 200\n",
        "\n",
        "text = data.Field(sequential = True, fix_length = max_len, batch_first = True, lower = True, dtype = torch.long)\n",
        "label = data.LabelField(sequential = False, dtype = torch.long)\n",
        "datasets.IMDB.download('./')\n",
        "\n",
        "ds_train, ds_test = datasets.IMDB.splits(text, label, path = './imdb/aclImdb')\n",
        "\n",
        "print('Train : ', len(ds_train))\n",
        "print('Test : ', len(ds_test))\n",
        "print('train.fields : ', ds_train.fields)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:09<00:00, 8.98MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train :  25000\n",
            "Test :  25000\n",
            "train.fields :  {'text': <torchtext.data.field.Field object at 0x7fbc13d3f278>, 'label': <torchtext.data.field.LabelField object at 0x7fbc13d3f2b0>}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOFDyXZ1IHg5",
        "outputId": "572d7c66-ffa4-4430-e5a6-65e243c9b81d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ds_train, ds_valid = ds_train.split(0.9)\n",
        "\n",
        "print('Train : ', len(ds_train))\n",
        "print('Validation : ', len(ds_valid))\n",
        "print('Test : ', len(ds_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train :  22500\n",
            "Validation :  2500\n",
            "Test :  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spdg1I6eIX-f"
      },
      "source": [
        "num_words = 50_000\n",
        "text.build_vocab(ds_train, max_size = num_words)\n",
        "label.build_vocab(ds_train)\n",
        "vocab = text.vocab"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G7G2ZMZIkZj"
      },
      "source": [
        "batch_size = 164\n",
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size = batch_size, sort_key = lambda x : len(x.text), repeat = False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2ld-7e4I9A3"
      },
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size, num_answers):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size, maximum_position_encoding = 10000)\n",
        "        self.dense = nn.Linear(d_model, num_answers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.encoder(x)\n",
        "\n",
        "        x, _ = torch.max(x, dim = 1)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cJsh_VrLQak",
        "outputId": "420f2123-d0d7-4c1a-d663-f12f308d202c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = TransformerClassifier(num_layers = 1, d_model = 32, num_heads = 2, conv_hidden_dim = 128, input_vocab_size = 50002, num_answers = 2)\n",
        "model.to(device)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerClassifier(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embeddings(\n",
              "      (word_embeddings): Embedding(50002, 32, padding_idx=1)\n",
              "      (position_embeddings): Embedding(10000, 32)\n",
              "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (enc_layers): ModuleList(\n",
              "      (0): EncodeLayer(\n",
              "        (mha): MultiHeadAttention(\n",
              "          (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "        (cnn): CNN(\n",
              "          (k1convL1): Linear(in_features=32, out_features=128, bias=True)\n",
              "          (k1convL2): Linear(in_features=128, out_features=32, bias=True)\n",
              "          (activation): ReLU()\n",
              "        )\n",
              "        (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oj-3DqRnLcEg"
      },
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001)\n",
        "epochs = 10\n",
        "t_total = len(train_loader) * epochs"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw2_g9uwL4Zi"
      },
      "source": [
        "def train(train_loader, valid_loader):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
        "        nb_batches_train = len(train_loader)\n",
        "        train_acc = 0\n",
        "        model.train()\n",
        "        losses = 0.0\n",
        "\n",
        "        for batch in train_iterator:\n",
        "\n",
        "            x = batch.text.to(device)\n",
        "            y = batch.label.to(device)\n",
        "\n",
        "            out = model(x)\n",
        "\n",
        "            loss = f.cross_entropy(out, y)\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            losses += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
        "\n",
        "        print(f'Training loss at epoch {epoch} is {losses / nb_batches_train}')\n",
        "        print(f'Training accuracy : {train_acc / nb_batches_train}')\n",
        "        print('Evaluating on validation')\n",
        "        evaluate(valid_loader)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGZV48JeMSbC"
      },
      "source": [
        "def evaluate(data_loader):\n",
        "\n",
        "    data_iterator = iter(data_loader)\n",
        "    nb_batches = len(data_loader)\n",
        "    model.eval()\n",
        "    acc = 0\n",
        "\n",
        "    for batch in data_iterator : \n",
        "\n",
        "        x = batch.text.to(device)\n",
        "        y = batch.label.to(device)\n",
        "\n",
        "        out = model(x)\n",
        "        acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
        "\n",
        "    print(f'Eval accuracy : {acc / nb_batches}')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7zv30LuNZGB",
        "outputId": "2fc11477-9aa3-479a-ae12-9e168e9f47f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train(train_loader, valid_loader)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training loss at epoch 0 is 0.6815453564775162\n",
            "Training accuracy : 0.5639967744786144\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.6300685975609756\n",
            "Training loss at epoch 1 is 0.6111233126426089\n",
            "Training accuracy : 0.6740732149169321\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.7048018292682927\n",
            "Training loss at epoch 2 is 0.5283002438752548\n",
            "Training accuracy : 0.7448248055850126\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.7245045731707315\n",
            "Training loss at epoch 3 is 0.451469285764556\n",
            "Training accuracy : 0.7948756185931424\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.7676448170731707\n",
            "Training loss at epoch 4 is 0.37960064195204474\n",
            "Training accuracy : 0.8336205372923293\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.7825457317073169\n",
            "Training loss at epoch 5 is 0.32337034450492996\n",
            "Training accuracy : 0.8649920466595967\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.7959222560975611\n",
            "Training loss at epoch 6 is 0.2702807648026425\n",
            "Training accuracy : 0.8940604012018385\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.8046874999999999\n",
            "Training loss at epoch 7 is 0.22758522629737854\n",
            "Training accuracy : 0.9146286231884058\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.8130716463414634\n",
            "Training loss at epoch 8 is 0.18802116125606108\n",
            "Training accuracy : 0.9315405178508306\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.8100228658536587\n",
            "Training loss at epoch 9 is 0.15290351341600003\n",
            "Training accuracy : 0.948076838105337\n",
            "Evaluating on validation\n",
            "Eval accuracy : 0.8165396341463416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kBWK06bNbJC",
        "outputId": "7ea44771-8156-44cd-d455-a12dcbf87fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluate(test_loader)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval accuracy : 0.804866978408346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMfNVVSxNnBx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}