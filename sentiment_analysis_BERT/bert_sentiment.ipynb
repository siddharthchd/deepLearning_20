{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddharthchd/deepLearning_20/blob/main/sentiment_analysis_BERT/bert_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGnlRWvkY-2c"
      },
      "source": [
        "In this problem we will use the BERT model for sentiment analysis. We will start with a pre-trained BERT model and fine-tune it on a dataset of Google Play store reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmj22-TcZMef"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install [the Transformers library](https://huggingface.co/transformers/) by Hugging Face:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj_7Tz0-pK69"
      },
      "source": [
        "!pip install -q -U watermark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjsbi1u3QFEM"
      },
      "source": [
        "!pip install -qq transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJqoaFpVpoM8",
        "outputId": "b7171a18-7b2b-449d-fcf4-ccc03ec25372",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPython 3.6.9\n",
            "IPython 5.5.0\n",
            "\n",
            "numpy 1.18.5\n",
            "pandas 1.1.3\n",
            "torch 1.6.0+cu101\n",
            "transformers 3.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufzPdoTtNikq"
      },
      "source": [
        "## Data Exploration\n",
        "\n",
        "Download the Google Play app reviews dataset using the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgPRhuMzi9ot",
        "outputId": "ac16541a-07ac-4d3d-a651-6d1abe7815aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
        "!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
            "To: /content/apps.csv\n",
            "100% 134k/134k [00:00<00:00, 4.34MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv\n",
            "To: /content/reviews.csv\n",
            "7.17MB [00:00, 63.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7GO8vXo6IVO"
      },
      "source": [
        "Here is how it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUKLyKc7I6Qp",
        "outputId": "6f2f8e28-59da-468e-e707-9214b0cf2f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import seaborn as sns\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import defaultdict\n",
        "\n",
        "df = pd.read_csv(\"reviews.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userName</th>\n",
              "      <th>userImage</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>thumbsUpCount</th>\n",
              "      <th>reviewCreatedVersion</th>\n",
              "      <th>at</th>\n",
              "      <th>replyContent</th>\n",
              "      <th>repliedAt</th>\n",
              "      <th>sortOrder</th>\n",
              "      <th>appId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Andrew Thomas</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
              "      <td>Update: After getting a response from the deve...</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-05 22:25:57</td>\n",
              "      <td>According to our TOS, and the term you have ag...</td>\n",
              "      <td>2020-04-05 15:10:24</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Craig Haines</td>\n",
              "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
              "      <td>Used it for a fair amount of time without any ...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-04 13:40:01</td>\n",
              "      <td>It sounds like you logged in with a different ...</td>\n",
              "      <td>2020-04-05 15:11:35</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>steven adkins</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
              "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-01 16:18:13</td>\n",
              "      <td>This sounds odd! We are not aware of any issue...</td>\n",
              "      <td>2020-04-02 16:05:56</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lars Panzerbj√∏rn</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
              "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-12 08:17:34</td>\n",
              "      <td>We do offer this option as part of the Advance...</td>\n",
              "      <td>2020-03-15 06:20:13</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scott Prewitt</td>\n",
              "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
              "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-14 17:41:01</td>\n",
              "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
              "      <td>2020-03-15 23:45:51</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           userName  ...      appId\n",
              "0     Andrew Thomas  ...  com.anydo\n",
              "1      Craig Haines  ...  com.anydo\n",
              "2     steven adkins  ...  com.anydo\n",
              "3  Lars Panzerbj√∏rn  ...  com.anydo\n",
              "4     Scott Prewitt  ...  com.anydo\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AiAdQ3j6SDe"
      },
      "source": [
        "Let's first check the size of the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB2jE6am7Dpo",
        "outputId": "70d850d9-cef6-4a66-9bd2-c772f7e276cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Q1. How many samples are there in this dataset? \n",
        "n_samples = df.shape[0]\n",
        "print('Number of samples in the dataset : {}'.format(n_samples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples in the dataset : 15746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwh_rW4Efhs3",
        "outputId": "b1a7f9de-55b7-435e-9b4d-76e77cd99e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "# TODO: Q2. Plot a histogram of review scores. These can be accessed in the df.score field in the above dataframe. Which score is the most common?\n",
        "df.hist(column = 'score', bins = 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7ff413b9b198>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZM0GKviobjM"
      },
      "source": [
        "If correctly plotted, you should be able to see that this is a somewhat imbalanced dataset. Let's first convert the dataset into three classes: negative, neutral, and positive sentiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei0xmdi1Chp0"
      },
      "source": [
        "def to_sentiment(rating):\n",
        "  rating = int(rating)\n",
        "  if rating <= 2:\n",
        "    return 0\n",
        "  elif rating == 3:\n",
        "    return 1\n",
        "  else: \n",
        "    return 2\n",
        "\n",
        "df['sentiment'] = df.score.apply(to_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-155O-SFSqE"
      },
      "source": [
        "class_names = ['negative', 'neutral', 'positive']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3tY3ECJDPaz",
        "outputId": "44c52a78-3894-4e21-9636-e1632dc1fbea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# TODO: Q3. Plot the histogram of review sentiments, and show that it is now approximately balanced.\n",
        "df['sentiment'].value_counts().plot(kind = 'bar', xlabel = class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7ff413488940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aHyGuTFgyPO"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Let's now load a pre-trained BERT model and the corresponding tokenizer, which converts text data into tokens. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Mj-0ne--5t"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3AfJSZ8NNLF"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfrSbwTQ-wi_"
      },
      "source": [
        "Let's see how tokenization works. Here is the test sentence. Convert into tokens using the `tokenizer.tokenize` and `tokenizer.convert_tokens_to_ids` methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZMitwrqm2eb"
      },
      "source": [
        "sample_txt = 'Every day feels like the same during the lock down.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTFhpHpsoWO7",
        "outputId": "0f0d9970-608d-4166-856d-38dbcd6cb456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Q4. Print the tokens and token ids of the sample text above.\n",
        "\n",
        "tokens = tokenizer.tokenize(sample_txt)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print('Text : {}'.format(sample_txt))\n",
        "print('Tokens : {}'.format(tokens))\n",
        "print('Token IDs : {}'.format(token_ids))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text : Every day feels like the same during the lock down.\n",
            "Tokens : ['Every', 'day', 'feels', 'like', 'the', 'same', 'during', 'the', 'lock', 'down', '.']\n",
            "Token IDs : [4081, 1285, 5115, 1176, 1103, 1269, 1219, 1103, 5842, 1205, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ap7jdL0LYU"
      },
      "source": [
        "BERT has special tokens for sentence separators \\[SEP\\] and unknown words \\[UNK\\]. This can be done using the [`encode_plus()`](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus) method, which takes the test sentence and encodes it into `input_ids`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vea9edaaxSPO",
        "outputId": "29c62e6a-bd60-4563-91a8-64185e5f8a9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  sample_txt,\n",
        "  max_length=32,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS69c8WvdOED"
      },
      "source": [
        "The token ids are now stored in a Tensor and padded to a length of 32:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzBmcOla0yQR",
        "outputId": "182ca5a0-f98d-4c92-a633-e696b0687f7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 101, 4081, 1285, 5115, 1176, 1103, 1269, 1219, 1103, 5842, 1205,  119,\n",
              "         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itAyVPsNdyc1"
      },
      "source": [
        "The attention mask has the same length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wiv5LLiw03Ox",
        "outputId": "66df78a7-70a1-482c-d09f-b80f647fb075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RvhC4jNHHy"
      },
      "source": [
        "Use the `tokenizer.convert_ids_to_tokens` method to invert the encoded token ids (the above tensor of length 32) and visualize the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IagGoafKLUwW",
        "outputId": "07571685-138c-4279-f44e-c46959e6b46f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Q5. Invert the encoded token ids.\n",
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'Every',\n",
              " 'day',\n",
              " 'feels',\n",
              " 'like',\n",
              " 'the',\n",
              " 'same',\n",
              " 'during',\n",
              " 'the',\n",
              " 'lock',\n",
              " 'down',\n",
              " '.',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW6ajl30t6du"
      },
      "source": [
        "Most reviews in the dataset contain less than around 120 tokens, but let us choose a maximum length of 160."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7xSmJtLuoxW"
      },
      "source": [
        "MAX_LEN = 160"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvvcoU6nurHy"
      },
      "source": [
        "# Building the dataset\n",
        "\n",
        "Let's now create a dataset using the tokenizer. Here is some code that does this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2BPgRJ7YBK0"
      },
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2uwsvCYqDJK"
      },
      "source": [
        "The tokenizer is doing most of the heavy lifting for us. We also return the review texts, so it'll be easier to evaluate the predictions from our model. Let's split the data into 90-5-5 train-validation-test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-vWzoo81dvO",
        "outputId": "1db989b8-699e-4678-9c60-2141ca826bd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Q6. Create three data frames: df_train, df_val, df_train as above and print their shapes.\n",
        "df_train, df_test = train_test_split(df, test_size = 0.1)\n",
        "df_val, df_test = train_test_split(df_test, test_size = 0.5)\n",
        "\n",
        "print('Shape of Training DataFrame : {}'.format(df_train.shape))\n",
        "print('Shape of Validation DataFrame : {}'.format(df_val.shape))\n",
        "print('Shape of Testing DataFrame : {}'.format(df_test.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of Training DataFrame : (14171, 12)\n",
            "Shape of Validation DataFrame : (787, 12)\n",
            "Shape of Testing DataFrame : (788, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4tQ1x-vqNab"
      },
      "source": [
        "We also need to create a couple of data loaders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEGqcvkuOuTX"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.content.to_numpy(),\n",
        "    targets=df.sentiment.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vODDxMKsPHqI"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6dlOptwqlhF"
      },
      "source": [
        "Let's have a look at an example batch from our training data loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y93ldSN47FeT",
        "outputId": "de327a48-e0e3-43dc-c2df-897596881a0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data = next(iter(train_data_loader))\n",
        "data.keys()\n",
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 160])\n",
            "torch.Size([16, 160])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "440Nd31VTHER"
      },
      "source": [
        "Let's now load the basic [BertModel](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) and build our sentiment classifier on top of it. Load the model using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P41FayISNRI"
      },
      "source": [
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFE7YSbFdY4t"
      },
      "source": [
        "And encode our sample text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1aoFxbQSn15"
      },
      "source": [
        "last_hidden_state, pooled_output = bert_model(\n",
        "  input_ids=encoding['input_ids'], \n",
        "  attention_mask=encoding['attention_mask']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLLu8zmqbaHV"
      },
      "source": [
        "The `last_hidden_state` is the sequence of hidden states of the last layer of the model. The `pooled_output` can be thought of as a summary of the content in the test sentence. Try printing out the sizes of `last_hidden_state` and `pooled_output`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUJHXNpIbcci",
        "outputId": "eb075de6-1b63-4aa9-ba04-60e4fb4cc97d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Q7. Print the sizes of the hidden states and the pooled output.\n",
        "last_hidden_state_size = last_hidden_state.shape\n",
        "pooled_output_size = pooled_output.shape\n",
        "\n",
        "print('Size of last_hidden_state : {}'.format(last_hidden_state_size))\n",
        "print('Size of pooled_output : {}'.format(pooled_output_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of last_hidden_state : torch.Size([1, 32, 768])\n",
            "Size of pooled_output : torch.Size([1, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o_NiS3WgOFf"
      },
      "source": [
        "We can use all of this knowledge to create a classifier that uses the BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_mRflxPl32F"
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJg8m3NQJahc"
      },
      "source": [
        "Note that our sentiment classifier takes the BERT backbone and adds a dropout layer (for regularization) and a linear dense layer, which we train using cross-entropy. Let's create an instance and move it to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0yQnuSFsjDp"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCPCFDLlKIQd"
      },
      "source": [
        "We'll move the example batch of our training data to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7p__CqdaMO"
      },
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr1EgkEtKOIB"
      },
      "source": [
        "To get the predicted probabilities from our trained model, we'll apply the softmax function to the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rTCj46Zamry",
        "outputId": "a000f7ef-fd32-44f7-8298-deaed7544124",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "F.softmax(model(input_ids, attention_mask), dim=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4332, 0.2276, 0.3392],\n",
              "        [0.4204, 0.2348, 0.3448],\n",
              "        [0.4695, 0.2417, 0.2888],\n",
              "        [0.4401, 0.2456, 0.3144],\n",
              "        [0.3837, 0.3528, 0.2635],\n",
              "        [0.4849, 0.1741, 0.3410],\n",
              "        [0.5256, 0.2534, 0.2211],\n",
              "        [0.2738, 0.3532, 0.3730],\n",
              "        [0.3958, 0.2784, 0.3258],\n",
              "        [0.3484, 0.2871, 0.3645],\n",
              "        [0.2831, 0.2975, 0.4194],\n",
              "        [0.3587, 0.2957, 0.3457],\n",
              "        [0.4204, 0.1947, 0.3850],\n",
              "        [0.4631, 0.2850, 0.2519],\n",
              "        [0.4633, 0.3069, 0.2297],\n",
              "        [0.4822, 0.3055, 0.2123]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9xikRdtRN1N"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76g7FV85H-T8"
      },
      "source": [
        "To train the model, we will use the AdamW optimizer and a linear learning-rate scheduler with no warmup steps, along with the cross-entropy loss. Five epochs (full passes through the training data should be enough) should be enough, but you can experiment with more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v-ArJ2fCCcU"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8522g7JIu5J"
      },
      "source": [
        "\n",
        "Let's continue with writing a helper function for training our model for one epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzl9UhuNx1_Q"
      },
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    # TODO Q8. Complete the incomplete code snippets below to finish training.\n",
        "    \n",
        "    input_ids = d['input_ids'].to(device)\n",
        "    attention_mask = d['attention_mask'].to(device)\n",
        "    targets = d['targets'].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PniYIte0fr"
      },
      "source": [
        "Let's write another function that helps us evaluate the model on a given data loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXeRorVGIKre"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  # TODO: Q9. Reproduce the above code but only evaluate the model (without any weight updates).\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    \n",
        "    for d in data_loader:\n",
        "\n",
        "      input_ids = d['input_ids'].to(device)\n",
        "      attention_mask = d['attention_mask'].to(device)\n",
        "      targets = d['targets'].to(device)\n",
        "\n",
        "      outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "\n",
        "      _, preds = torch.max(outputs, dim =1)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_rdSDBHhhCh"
      },
      "source": [
        "Using those two, we can write our training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zhHoFNsxufs",
        "outputId": "5ecad44d-bf63-4c01-bdd3-17b92107ba4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  # TODO: Q10. Complete the code below to track train and test accuracy.losses\n",
        "\n",
        "  train_acc, train_loss = train_epoch(model = model, data_loader = train_data_loader, loss_fn = loss_fn, optimizer = optimizer, device = device, scheduler = scheduler, n_examples = len(df_train))\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(model = model, data_loader = val_data_loader, loss_fn = loss_fn, device = device, n_examples = len(df_val))\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.2116008722833011 accuracy 0.9357843483169854\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5737093920819462 accuracy 0.8500635324015248\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.14786053583174144 accuracy 0.9595653094347611\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5908328121318482 accuracy 0.8576874205844981\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.11579102072268241 accuracy 0.9702208736151295\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6543997432745527 accuracy 0.8627700127064803\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10765344892468452 accuracy 0.9711382400677441\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6543997432745527 accuracy 0.8627700127064803\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.10862428631447466 accuracy 0.9718439065697552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6543997432745527 accuracy 0.8627700127064803\n",
            "\n",
            "CPU times: user 21min 26s, sys: 14min 38s, total: 36min 5s\n",
            "Wall time: 36min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r8-5zWsiVur"
      },
      "source": [
        "Note that we're storing the best model, indicated by the highest validation accuracy.\n",
        "\n",
        "Plot train and validation accuracy as a function of epoch count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FWG7kBm372V",
        "outputId": "76591095-0a97-42bd-96e9-b544782e15ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# TODO: Q11. Plot train/validation accuracies.\n",
        "plt.plot(history['train_acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Epoch')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff417fade80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHqkLAuf8pv"
      },
      "source": [
        "You might try to fine-tune the parameters (learning rate, batch size) a bit more if accuracy is not good enough.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3HZb3NWFtFf"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "So how good is our model on predicting sentiment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdQ7-ylCj8Gd"
      },
      "source": [
        "We'll define a helper function to get the predictions from our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgR6MuNS8jr_"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkbnBTI7kd_y"
      },
      "source": [
        "This is similar to the evaluation function, except that we're storing the text of the reviews and the predicted probabilities (by applying the softmax on the model outputs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHdPZr60-0c_",
        "outputId": "2d8331f2-5934-449e-a959-ea3e50ef0e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFAekw3mmWUi"
      },
      "source": [
        "Let us compare true sentiment vs predicted sentiment by plotting a confusion matrix of `y_test` vs `y_pred`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d1qxsc__DTh",
        "outputId": "523a77ed-ada1-48d0-8f01-066c3f0c6b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# TODO. Q12. Plot the 3x3 confusion matrix and show that the model finds it a bit difficult to classify neutral reviews.\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "hmap = sns.heatmap(pd.DataFrame(cm, index = class_names, columns = class_names), annot = True, fmt = 'd')\n",
        "plt.xlabel('Predicted Sentiment')\n",
        "plt.ylabel('True Sentiment')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'True Sentiment')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WL5pDmvFyaU"
      },
      "source": [
        "### Predicting on Raw Text\n",
        "\n",
        "Let's use our model to predict the sentiment of some raw text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEPi7zQRsDhH"
      },
      "source": [
        "review_text = \"I love Deep Learning! Best course evah!!!1!!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et8xlDrKpH60"
      },
      "source": [
        "Use your trained model to predict the sentiment expressed in `review_text`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr_t3rUksumr",
        "outputId": "c7e578a2-a017-4ef4-b4e9-3f806113d4e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO: Q13. Print the predicted sentiment in `review_text`.\n",
        "\n",
        "review_encoded = tokenizer.encode_plus(review_text, max_length = MAX_LEN, add_special_tokens = True, return_token_type_ids = False, pad_to_max_length = True, return_attention_mask = True, return_tensors = 'pt')\n",
        "\n",
        "input_ids = review_encoded['input_ids'].to(device)\n",
        "attention_mask = review_encoded['attention_mask'].to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim = 1)\n",
        "\n",
        "print('Review Text : {}'.format(review_text))\n",
        "print('Sentiment : {}'.format(class_names[prediction]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review Text : I love Deep Learning! Best course evah!!!1!!\n",
            "Sentiment : positive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf39tauBa2V2"
      },
      "source": [
        "## References\n",
        "\n",
        "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
        "- [L11 Language Models - Alec Radford (OpenAI)](https://www.youtube.com/watch?v=BnpB3GrpsfM)\n",
        "- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)\n",
        "- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
        "- [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/pdf/1905.05583.pdf)\n",
        "- [Huggingface Transformers](https://huggingface.co/transformers/)\n",
        "- [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)"
      ]
    }
  ]
}