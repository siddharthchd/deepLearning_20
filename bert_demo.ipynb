{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP+cpzQgI4kUmDmLv9D9ChF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddharthchd/deepLearning_20/blob/main/bert_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzgJaxCZQDTG",
        "outputId": "bc0b8fb8-2c71-4c37-8e35-af65728313b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -U mxnet-cu101==1.7.0\n",
        "!pip install d2l==0.15.0"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: mxnet-cu101==1.7.0 in /usr/local/lib/python3.6/dist-packages (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (1.24.3)\n",
            "Requirement already satisfied: d2l==0.15.0 in /usr/local/lib/python3.6/dist-packages (0.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.0) (1.1.3)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.0) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.0) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.0) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.15.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.15.0) (2018.9)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.0) (4.10.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.0) (4.7.7)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.0) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.0) (5.3.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.0) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.0) (7.5.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.0) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.0) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->d2l==0.15.0) (1.15.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.15.0) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.15.0) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.15.0) (4.3.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.15.0) (5.3.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.0) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.0) (4.6.3)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.0) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.0) (19.0.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.0) (2.6.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.0) (3.2.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.0) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.0) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.0) (0.8.4)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.0) (5.0.8)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.0) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.0) (0.4.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.0) (2.11.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.0) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.0) (1.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.15.0) (1.0.18)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.15.0) (3.5.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.0) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.0) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.0) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.0) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.0) (50.3.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.0) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.0) (20.4)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->d2l==0.15.0) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->d2l==0.15.0) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.15.0) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.15.0) (0.2.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClcL1j-ySpM7"
      },
      "source": [
        "# Bidirectional Encoder Representations from Transformers (BERT)\n",
        ":label:`sec_bert`\n",
        "\n",
        "We have introduced several word embedding models for natural language understanding.\n",
        "After pretraining, the output can be thought of as a matrix\n",
        "where each row is a vector that represents a word of a predefined vocabulary.\n",
        "In fact, these word embedding models are all *context-independent*.\n",
        "Let us begin by illustrating this property.\n",
        "\n",
        "\n",
        "## From Context-Independent to Context-Sensitive\n",
        "\n",
        "Recall the experiments in :numref:`sec_word2vec_pretraining` and :numref:`sec_synonyms`.\n",
        "For instance, word2vec and GloVe both assign the same pretrained vector to the same word regardless of the context of the word (if any).\n",
        "Formally, a context-independent representation of any token $x$\n",
        "is a function $f(x)$ that only takes $x$ as its input.\n",
        "Given the abundance of polysemy and complex semantics in natural languages,\n",
        "context-independent representations have obvious limitations.\n",
        "For instance, the word \"crane\" in contexts\n",
        "\"a crane is flying\" and \"a crane driver came\" has completely different meanings;\n",
        "thus, the same word may be assigned different representations depending on contexts.\n",
        "\n",
        "This motivates the development of *context-sensitive* word representations,\n",
        "where representations of words depend on their contexts.\n",
        "Hence, a context-sensitive representation of token $x$ is a function $f(x, c(x))$\n",
        "depending on both $x$ and its context $c(x)$.\n",
        "Popular context-sensitive representations\n",
        "include TagLM (language-model-augmented sequence tagger) :cite:`Peters.Ammar.Bhagavatula.ea.2017`,\n",
        "CoVe (Context Vectors) :cite:`McCann.Bradbury.Xiong.ea.2017`,\n",
        "and ELMo (Embeddings from Language Models) :cite:`Peters.Neumann.Iyyer.ea.2018`.\n",
        "\n",
        "For example, by taking the entire sequence as the input,\n",
        "ELMo is a function that assigns a representation to each word from the input sequence.\n",
        "Specifically, ELMo combines all the intermediate layer representations from pretrained bidirectional LSTM as the output representation.\n",
        "Then the ELMo representation will be added to a downstream task's existing supervised model\n",
        "as additional features, such as by concatenating ELMo representation and the original representation (e.g., GloVe) of tokens in the existing model.\n",
        "On one hand,\n",
        "all the weights in the pretrained bidirectional LSTM model are frozen after ELMo representations are added.\n",
        "On the other hand,\n",
        "the existing supervised model is specifically customized for a given task.\n",
        "Leveraging different best models for different tasks at that time,\n",
        "adding ELMo improved the state of the art across six natural language processing tasks:\n",
        "sentiment analysis, natural language inference,\n",
        "semantic role labeling, coreference resolution,\n",
        "named entity recognition, and question answering.\n",
        "\n",
        "\n",
        "## From Task-Specific to Task-Agnostic\n",
        "\n",
        "Although ELMo has significantly improved solutions to a diverse set of natural language processing tasks,\n",
        "each solution still hinges on a *task-specific* architecture.\n",
        "However, it is practically non-trivial to craft a specific architecture for every natural language processing task.\n",
        "The GPT (Generative Pre-Training) model represents an effort in designing\n",
        "a general *task-agnostic* model for context-sensitive representations :cite:`Radford.Narasimhan.Salimans.ea.2018`.\n",
        "Built on a Transformer decoder,\n",
        "GPT pretrains a language model that will be used to represent text sequences.\n",
        "When applying GPT to a downstream task,\n",
        "the output of the language model will be fed into an added linear output layer\n",
        "to predict the label of the task.\n",
        "In sharp contrast to ELMo that freezes parameters of the pretrained model,\n",
        "GPT fine-tunes *all* the parameters in the pretrained Transformer decoder\n",
        "during supervised learning of the downstream task.\n",
        "GPT was evaluated on twelve tasks of natural language inference,\n",
        "question answering, sentence similarity, and classification,\n",
        "and improved the state of the art in nine of them with minimal changes\n",
        "to the model architecture.\n",
        "\n",
        "However, due to the autoregressive nature of language models,\n",
        "GPT only looks forward (left-to-right).\n",
        "In contexts \"i went to the bank to deposit cash\" and \"i went to the bank to sit down\",\n",
        "as \"bank\" is sensitive to the context to its left,\n",
        "GPT will return the same representation for \"bank\",\n",
        "though it has different meanings.\n",
        "\n",
        "\n",
        "## BERT: Combining the Best of Both Worlds\n",
        "\n",
        "As we have seen,\n",
        "ELMo encodes context bidirectionally but uses task-specific architectures;\n",
        "while GPT is task-agnostic but encodes context left-to-right.\n",
        "Combining the best of both worlds,\n",
        "BERT (Bidirectional Encoder Representations from Transformers)\n",
        "encodes context bidirectionally and requires minimal architecture changes\n",
        "for a wide range of natural language processing tasks :cite:`Devlin.Chang.Lee.ea.2018`.\n",
        "Using a pretrained Transformer encoder,\n",
        "BERT is able to represent any token based on its bidirectional context.\n",
        "During supervised learning of downstream tasks,\n",
        "BERT is similar to GPT in two aspects.\n",
        "First, BERT representations will be fed into an added output layer,\n",
        "with minimal changes to the model architecture depending on nature of tasks,\n",
        "such as predicting for every token vs. predicting for the entire sequence.\n",
        "Second,\n",
        "all the parameters of the pretrained Transformer encoder are fine-tuned,\n",
        "while the additional output layer will be trained from scratch.\n",
        ":numref:`fig_elmo-gpt-bert` depicts the differences among ELMo, GPT, and BERT.\n",
        "\n",
        "![A comparison of ELMo, GPT, and BERT.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/elmo-gpt-bert.svg?raw=1)\n",
        ":label:`fig_elmo-gpt-bert`\n",
        "\n",
        "\n",
        "BERT further improved the state of the art on eleven natural language processing tasks\n",
        "under broad categories of i) single text classification (e.g., sentiment analysis), ii) text pair classification (e.g., natural language inference),\n",
        "iii) question answering, iv) text tagging (e.g., named entity recognition).\n",
        "All proposed in 2018,\n",
        "from context-sensitive ELMo to task-agnostic GPT and BERT,\n",
        "conceptually simple yet empirically powerful pretraining of deep representations for natural languages have revolutionized solutions to various natural language processing tasks.\n",
        "\n",
        "In the rest of this chapter,\n",
        "we will dive into the pretraining of BERT.\n",
        "When natural language processing applications are explained in :numref:`chap_nlp_app`,\n",
        "we will illustrate fine-tuning of BERT for downstream applications.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfj12gy1S_Pp"
      },
      "source": [
        "from d2l import mxnet as d2l\n",
        "from mxnet import gluon, np, npx\n",
        "from mxnet.gluon import nn\n",
        "\n",
        "npx.set_np()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7ZTxTncU8jB"
      },
      "source": [
        "## Input Representation\n",
        ":label:`subsec_bert_input_rep`\n",
        "\n",
        "In natural language processing,\n",
        "some tasks (e.g., sentiment analysis) take single text as the input,\n",
        "while in some other tasks (e.g., natural language inference),\n",
        "the input is a pair of text sequences.\n",
        "The BERT input sequence unambiguously represents both single text and text pairs.\n",
        "In the former,\n",
        "the BERT input sequence is the concatenation of\n",
        "the special classification token “&lt;cls&gt;”,\n",
        "tokens of a text sequence,\n",
        "and the special separation token “&lt;sep&gt;”.\n",
        "In the latter,\n",
        "the BERT input sequence is the concatenation of\n",
        "“&lt;cls&gt;”, tokens of the first text sequence,\n",
        "“&lt;sep&gt;”, tokens of the second text sequence, and “&lt;sep&gt;”.\n",
        "We will consistently distinguish the terminology \"BERT input sequence\"\n",
        "from other types of \"sequences\".\n",
        "For instance, one *BERT input sequence* may include either one *text sequence* or two *text sequences*.\n",
        "\n",
        "To distinguish text pairs,\n",
        "the learned segment embeddings $\\mathbf{e}_A$ and $\\mathbf{e}_B$\n",
        "are added to the token embeddings of the first sequence and the second sequence, respectively.\n",
        "For single text inputs, only $\\mathbf{e}_A$ is used.\n",
        "\n",
        "The following `get_tokens_and_segments` takes either one sentence or two sentences\n",
        "as the input, then returns tokens of the BERT input sequence\n",
        "and their corresponding segment IDs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiccF1A3WTvO"
      },
      "source": [
        "#@save\n",
        "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
        "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
        "    # 0 and 1 are marking segment A and B, respectively\n",
        "    segments = [0] * (len(tokens_a) + 2)\n",
        "    if tokens_b is not None:\n",
        "        tokens += tokens_b + ['<sep>']\n",
        "        segments += [1] * (len(tokens_b) + 1)\n",
        "    return tokens, segments"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD77Jnt0W3xY"
      },
      "source": [
        "BERT chooses the Transformer encoder as its bidirectional architecture.\n",
        "Common in the Transformer encoder,\n",
        "positional embeddings are added at every position of the BERT input sequence.\n",
        "However, different from the original Transformer encoder,\n",
        "BERT uses *learnable* positional embeddings.\n",
        "To sum up, :numref:`fig_bert-input` shows that\n",
        "the embeddings of the BERT input sequence are the sum\n",
        "of the token embeddings, segment embeddings, and positional embeddings.\n",
        "\n",
        "![The embeddings of the BERT input sequence are the sum\n",
        "of the token embeddings, segment embeddings, and positional embeddings.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/bert-input.svg?raw=1)\n",
        ":label:`fig_bert-input`\n",
        "\n",
        "The following `BERTEncoder` class is similar to the `TransformerEncoder` class\n",
        "as implemented in :numref:`sec_transformer`.\n",
        "Different from `TransformerEncoder`, `BERTEncoder` uses\n",
        "segment embeddings and learnable positional embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiqprXGFW7Sw"
      },
      "source": [
        "#@save\n",
        "class BERTEncoder(nn.Block):\n",
        "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
        "                 num_layers, dropout, max_len=1000, **kwargs):\n",
        "        super(BERTEncoder, self).__init__(**kwargs)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens) # lookup table that stores embeddings of a fixed dictionary and size\n",
        "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
        "        self.blks = nn.Sequential()\n",
        "        for _ in range(num_layers):\n",
        "            self.blks.add(d2l.EncoderBlock(\n",
        "                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n",
        "            \n",
        "        # In BERT, positional embeddings are learnable, thus we create a\n",
        "        # parameter of positional embeddings that are long enough\n",
        "        self.pos_embedding = self.params.get('pos_embedding',\n",
        "                                             shape=(1, max_len, num_hiddens))\n",
        "\n",
        "    def forward(self, tokens, segments, valid_lens):\n",
        "        # Shape of `X` remains unchanged in the following code snippet:\n",
        "        # (batch size, max sequence length, `num_hiddens`)\n",
        "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
        "        X = X + self.pos_embedding.data(ctx=X.ctx)[:, :X.shape[1], :]\n",
        "        for blk in self.blks:\n",
        "            X = blk(X, valid_lens)\n",
        "        return X"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "3"
        },
        "origin_pos": 7,
        "tab": [
          "mxnet"
        ],
        "id": "ZpVD_hVRTq64"
      },
      "source": [
        "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
        "num_layers, dropout = 2, 0.2\n",
        "encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
        "                      num_layers, dropout)\n",
        "encoder.initialize()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFI7nGMcbSuH",
        "outputId": "205574a6-e9ec-42fd-c39e-cb5eb6ee2fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokens = np.random.randint(0, vocab_size, (2, 8))\n",
        "segments = np.array([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
        "encoded_X = encoder(tokens, segments, None)\n",
        "encoded_X.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 8, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrnTTy72bYwS"
      },
      "source": [
        "## Pretraining Tasks\n",
        ":label:`subsec_bert_pretraining_tasks`\n",
        "\n",
        "The forward inference of `BERTEncoder` gives the BERT representation\n",
        "of each token of the input text and the inserted\n",
        "special tokens “&lt;cls&gt;” and “&lt;seq&gt;”.\n",
        "Next, we will use these representations to compute the loss function\n",
        "for pretraining BERT.\n",
        "The pretraining is composed of the following two tasks:\n",
        "masked language modeling and next sentence prediction.\n",
        "\n",
        "### Masked Language Modeling\n",
        ":label:`subsec_mlm`\n",
        "\n",
        "As illustrated in :numref:`sec_language_model`,\n",
        "a language model predicts a token using the context on its left.\n",
        "To encode context bidirectionally for representing each token,\n",
        "BERT randomly masks tokens and uses tokens from the bidirectional context to\n",
        "predict the masked tokens.\n",
        "This task is referred to as a *masked language model*.\n",
        "\n",
        "In this pretraining task,\n",
        "15% of tokens will be selected at random as the masked tokens for prediction.\n",
        "To predict a masked token without cheating by using the label,\n",
        "one straightforward approach is to always replace it with a special “&lt;mask&gt;” token in the BERT input sequence.\n",
        "However, the artificial special token “&lt;mask&gt;” will never appear\n",
        "in fine-tuning.\n",
        "To avoid such a mismatch between pretraining and fine-tuning,\n",
        "if a token is masked for prediction (e.g., \"great\" is selected to be masked and predicted in \"this movie is great\"),\n",
        "in the input it will be replaced with:\n",
        "\n",
        "* a special “&lt;mask&gt;” token for 80% of the time (e.g., \"this movie is great\" becomes \"this movie is &lt;mask&gt;\");\n",
        "* a random token for 10% of the time (e.g., \"this movie is great\" becomes \"this movie is drink\");\n",
        "* the unchanged label token for 10% of the time (e.g., \"this movie is great\" becomes \"this movie is great\").\n",
        "\n",
        "Note that for 10% of 15% time a random token is inserted.\n",
        "This occasional noise encourages BERT to be less biased towards the masked token (especially when the label token remains unchanged) in its bidirectional context encoding.\n",
        "\n",
        "We implement the following `MaskLM` class to predict masked tokens\n",
        "in the masked language model task of BERT pretraining.\n",
        "The prediction uses a one-hidden-layer MLP (`self.mlp`).\n",
        "In forward inference, it takes two inputs:\n",
        "the encoded result of `BERTEncoder` and the token positions for prediction.\n",
        "The output is the prediction results at these positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3YQJCgfbbID"
      },
      "source": [
        "#@save\n",
        "class MaskLM(nn.Block):\n",
        "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
        "        super(MaskLM, self).__init__(**kwargs)\n",
        "        self.mlp = nn.Sequential()\n",
        "        self.mlp.add(\n",
        "            nn.Dense(num_hiddens, flatten=False, activation='relu'))\n",
        "        self.mlp.add(nn.LayerNorm())\n",
        "        self.mlp.add(nn.Dense(vocab_size, flatten=False))\n",
        "\n",
        "    def forward(self, X, pred_positions):\n",
        "        num_pred_positions = pred_positions.shape[1]\n",
        "        pred_positions = pred_positions.reshape(-1)\n",
        "        batch_size = X.shape[0]\n",
        "        batch_idx = np.arange(0, batch_size)\n",
        "        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n",
        "        # `batch_idx` is `np.array([0, 0, 0, 1, 1, 1])`\n",
        "        batch_idx = np.repeat(batch_idx, num_pred_positions)\n",
        "        masked_X = X[batch_idx, pred_positions]\n",
        "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
        "        mlm_Y_hat = self.mlp(masked_X)\n",
        "        return mlm_Y_hat"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfduh-AKcGGi",
        "outputId": "a09eb33e-ea49-44c4-fc50-c59ecfe94919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mlm = MaskLM(vocab_size, num_hiddens)\n",
        "mlm.initialize()\n",
        "mlm_positions = np.array([[1, 5, 2], [6, 1, 5]])\n",
        "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
        "mlm_Y_hat.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw5vP7Fsc8nm",
        "outputId": "c4af6608-1cda-4b97-94ca-407dacfab961",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "mlm_Y = np.array([[7, 8, 9], [10, 20, 30]])\n",
        "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
        "mlm_l.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDZM3aU1c-KV"
      },
      "source": [
        "### Next Sentence Prediction\n",
        ":label:`subsec_nsp`\n",
        "\n",
        "Although masked language modeling is able to encode bidirectional context\n",
        "for representing words, it does not explicitly model the logical relationship\n",
        "between text pairs.\n",
        "To help understand the relationship between two text sequences,\n",
        "BERT considers a binary classification task, *next sentence prediction*, in its pretraining.\n",
        "When generating sentence pairs for pretraining,\n",
        "for half of the time they are indeed consecutive sentences with the label \"True\";\n",
        "while for the other half of the time the second sentence is randomly sampled from the corpus with the label \"False\".\n",
        "\n",
        "The following `NextSentencePred` class uses a one-hidden-layer MLP\n",
        "to predict whether the second sentence is the next sentence of the first\n",
        "in the BERT input sequence.\n",
        "Due to self-attention in the Transformer encoder,\n",
        "the BERT representation of the special token “&lt;cls&gt;”\n",
        "encodes both the two sentences from the input.\n",
        "Hence, the output layer (`self.output`) of the MLP classifier takes `X` as the input,\n",
        "where `X` is the output of the MLP hidden layer whose input is the encoded “&lt;cls&gt;” token.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KFzAlohdALA"
      },
      "source": [
        "#@save\n",
        "class NextSentencePred(nn.Block):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NextSentencePred, self).__init__(**kwargs)\n",
        "        self.output = nn.Dense(2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # `X` shape: (batch size, `num_hiddens`)\n",
        "        return self.output(X)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTTASQ8TdOj5",
        "outputId": "f62c301b-136e-4c9e-f42c-84369df20f2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nsp = NextSentencePred()\n",
        "nsp.initialize()\n",
        "nsp_Y_hat = nsp(encoded_X)\n",
        "nsp_Y_hat.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYhu68URdQtm",
        "outputId": "a7b087cf-4f37-4fa2-96f3-4036ea10cb2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nsp_y = np.array([0, 1])\n",
        "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
        "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
        "nsp_l.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYDPP2rWdS2v"
      },
      "source": [
        "It is noteworthy that all the labels in both the aforementioned pretraining tasks\n",
        "can be trivially obtained from the pretraining corpus without manual labeling effort.\n",
        "The original BERT has been pretrained on the concatenation of BookCorpus :cite:`Zhu.Kiros.Zemel.ea.2015`\n",
        "and English Wikipedia.\n",
        "These two text corpora are huge:\n",
        "they have 800 million words and 2.5 billion words, respectively.\n",
        "\n",
        "\n",
        "## Putting All Things Together\n",
        "\n",
        "When pretraining BERT, the final loss function is a linear combination of\n",
        "both the loss functions for masked language modeling and next sentence prediction.\n",
        "Now we can define the `BERTModel` class by instantiating the three classes\n",
        "`BERTEncoder`, `MaskLM`, and `NextSentencePred`.\n",
        "The forward inference returns the encoded BERT representations `encoded_X`,\n",
        "predictions of masked language modeling `mlm_Y_hat`,\n",
        "and next sentence predictions `nsp_Y_hat`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFIx6GOYdWfJ"
      },
      "source": [
        "#@save\n",
        "class BERTModel(nn.Block):\n",
        "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
        "                 num_layers, dropout, max_len=1000):\n",
        "        super(BERTModel, self).__init__()\n",
        "        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n",
        "                                   num_heads, num_layers, dropout, max_len)\n",
        "        self.hidden = nn.Dense(num_hiddens, activation='tanh')\n",
        "        self.mlm = MaskLM(vocab_size, num_hiddens)\n",
        "        self.nsp = NextSentencePred()\n",
        "\n",
        "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
        "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
        "        if pred_positions is not None:\n",
        "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
        "        else:\n",
        "            mlm_Y_hat = None\n",
        "        # The hidden layer of the MLP classifier for next sentence prediction.\n",
        "        # 0 is the index of the '<cls>' token\n",
        "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
        "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fABFhot0dhWR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}